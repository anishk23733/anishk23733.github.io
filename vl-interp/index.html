<!DOCTYPE html>
<html lang="en" class="fontawesome-i2svg-active fontawesome-i2svg-complete">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <script type="text/javascript"
  src="https://www.maths.nottingham.ac.uk/plp/pmadw/LaTeXMathML.js">
  </script>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations</title>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6">
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
      <script defer="" src="css/fontawesome.all.min.js"></script>
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <style>
      body {
        font-size:20px;
        margin:60px auto;
        width:auto;
        max-width:900px;
      }

      hr {
        border:0;
        height:1.0px;
        background-image:linear-gradient(to right, rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3));
      }

      .gap-30 {
      width:100%;
      height:30px;
      }

      .gap-20 {
      width:100%;
      height:20px;
      }

      .gap-10 {
      width:100%;
      height:10px;
      }

      .gap-5 {
      width:100%;
      height:5px;
      }
      .paper {
        max-width: 700px;
      }
      @media (max-width: 910px) {
        .paper {
          max-width: 500px;
        }
      }
      @media (max-width: 610px) {
        .paper {
          max-width: 300px;
        }
      }
    </style>
  </head>

  <div class="container">
  <body>

    <center><span style="font-size:40px">
        Interpreting and Editing Vision-Language<br>Representations to Mitigate Hallucinations
    </span></center>
    <div class="gap-20"></div>

    <!---------------------  authors --------------------->
    <span style="font-size:20px">
    <div class="row">
      <div class="col-md-3">
        <center><a href="https://nickjiang.me/"><span style="font-size:21px">Nick Jiang<sup>*</sup></span></a>
      </div>
      <div class="col-md-3">
        <center><a href="https://anishk.me"><span style="font-size:21px">Anish Kachinthaya<sup>*</sup></span></a>
      </div>
      <div class="col-md-3">
        <center><a href="https://suziepetryk.com/"><span style="font-size:21px">Suzie Petryk<sup>†</sup></span></a>
      </div>
      <div class="col-md-3">
        <center><a href="https://yossigandelsman.github.io/"><span style="font-size:21px">Yossi Gandelsman<sup>†</sup></span></a>
      </div>
    </div>
  </span>
    <!--------------------- affiliations --------------------->
    <div class="gap-5"></div>
    
    <div class="row">
      <div class="col-md-12">
        <center>
            <span style="font-size:18px">UC Berkeley</span>
        </center>
      </div>      
    </div>
    <div class="row">
      <div class="col-md-12">
        <center>
            <span style="font-size:14px"><sup>*†</sup>Equal contribution</span>
        </center>
      </div>      
    </div>
    <div class="gap-10"></div>
    <hr>

    <!--------------------- links --------------------->
    <div class="gap-10"></div>

    <center>
    <span style="font-size:23px">
      [<a href="https://arxiv.org/abs/2410.02762" class="external-link button is-normal is-rounded is-dark">paper</a>]
      &nbsp;
      [<a href="bibtex.txt">bibtex</a>]
      &nbsp; 
      [<a href="https://github.com/nickjiang2378/vl-interp/">code</a>]&nbsp; 
    </span>
    <br>
    <!-- <b>Conference Name</b> (Oral Presentation) -->
    <div class="gap-20"></div>
    </center>
<img width="100%" src="images/teaser.png">
<p></p>
<p><i><b>Interpreting VLM internal image representations.</b> (a) Given a VLM, (b) we unembed the latent representations from image embeddings to the vocabulary and classify hallucinations. We remove hallucinations by (c) linearly editing them out of the latent representations.
  </i></p>
<hr>
    <!--------------------- abstract --------------------->
    <div class="gap-20"></div>
    <center><b><span style="font-size:25px">Abstract</span></b><br></center>
    <div class="gap-10"></div>
    <p> 
        We investigate the internal representations of vision-language models (VLMs) to address hallucinations, a persistent challenge despite advances in model size and training. We project VLMs' internal image representations to their language vocabulary and identify differences in token output probabilities between real and hallucinated objects. We additionally use these output probabilities to spatially localize real objects. Building on this approach, we introduce a knowledge erasure algorithm that removes hallucinations by linearly orthogonalizing image features with respect to hallucinated object features. We show that targeted edits to a model's latent representations can reduce hallucinations by up to 25.7% on the COCO2014 dataset while preserving performance. Our findings demonstrate how a deeper understanding of VLMs' latent representations can enhance reliability and enable novel capabilities, such as zero-shot segmentation.    </p>
    <hr>
    <!--------------------- content --------------------->
    <!-- <div class="gap-20"></div> -->
    <b><span style="font-size:25px">Interpreting Image Representations with Logit Lens</span></b>
    <div class="gap-10"></div>
    <p>
        We apply the <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens">logit lens</a> to probe the language model as it processes the image representations in two VLMs, <a href="https://arxiv.org/abs/2305.06500">InstructBLIP</a> and <a href="https://llava-vl.github.io/">LLaVA</a>.
        This enables us to interpret the image features’ output distributions as they are transformed by the layers of the language model and localize objects spatially within the image. 
        For an input embedding $t_m$, given its hidden representation $h_l(t_m)$ and the language model's unembedding matrix $W_U$, we obtain the logits output by the language model at layer $l$ as follows:
    </p>
    <center>
    <blockquote>$$f_l(t_{m}) = W_U \cdot h_l(t_m) = [\text{logit}_1, \text{logit}_2, \text{logit}_3, \ldots, \text{logit}_{|V|}]$$</blockquote>
  </center>
    <p>The logits from the layer can be decomposed further for a specific object $o$ to find the maximum probability of that object over all intermediate representations of image embeddings $k_i$  over all layers $l$:</p>
    <center>
    <blockquote>$$c_o = \max_{\substack{1 \leq l \leq L \\ 1 \leq i \leq n}} \{\text{softmax}(f_l(k_i))_o\}$$</blockquote>
  </center>
    <p>We define $c_o$ as the VLMs <strong>internal confidence</strong> of an object $o$ existing in the image: the highest probability of object presence across $n$ image representations through $L$ layers of the language model.
    </p>

    <br>

    <img width="100%" src="images/internal_confidence.png">
    <p></p>
    <p><i><b>Localizing objects using internal confidence values.</b> We find the probabilities of objects through layers of the language model for every image embedding in LLaVA. We use the highest layer probability per image embedding to localize an object within the image.
    </i></p>
    
    <hr>
    
    <b><span style="font-size:25px">Erasing knowledge from VLMs</span></b>
    <div class="gap-10"></div>
    <p>Recognizing that image embeddings are directly interpretable, we edit these embeddings to erase the presence of objects from image captions. We propose a linear editing algorithm that subtracts the text embedding of a target object from all image embeddings. When applied on singular and multiple object removals, we find that it erases hallucinated objects more effectively than correctly detected (CD) objects (i.e. real objects that the model correctly detects).</p>
    <center>
    <div style="width: 60%;">
        <img style="width: 100%;" src="images/projectaway.png">
        <p><code>ProjectAway</code>: Our editing algorithm erases the presence of an object from image embeddings by orthogonalizing them with respect to the object's text embedding.</p>
    </div>
    </center>

    <p>
        We apply <code>ProjectAway</code> for both individually removing objects and mass-removing objects. 
        When mass-removing hallucinations identified with ground truth annotations, we achieve a hallucination reduction rate of 41.3% for InstructBLIP and 23.3% for LLaVA. We similarly find that it can successfully remove correctly detected (CD) objects when edited together in a single image.
    </p>

    <div class="gap-10"></div>
    <img width="100%" src="images/removal_results.png">
    <p></p>
    <p><i><b>Removing mentioned objects individually and in-mass.</b>
        Using <code>ProjectAway</code>, we remove hallucinated objects and observe high hallucination reduction with <a href="https://arxiv.org/pdf/1809.02156">CHAIR</a>, mass-removal rate (Mass RR), and individual removal rate (Individual RR). We also remove correctly detected (CD) objects but find that they are more resistant to linear editing. 
        Denote CHAIR<sub>S</sub> as $C_S$ and CHAIR<sub>I</sub> as $C_I$.
    </i></p>

    <div class="gap-10"></div>
    <img width="100%" src="images/hallucination_removal_examples.png">
    <p></p>
    <p><i><b>Qualitative results for mass object removal.</b>
        We present example images and their captions after mass-removing hallucinations (<span style="color:red;">red</span>) with <code>ProjectAway</code>, which can effectively remove hallucinations while preserving, even increasing, correctly detected objects (<span style="color:green;">green</span>).
    </i></p>

    <hr>
    
    <b><span style="font-size:25px">Applications: Hallucination Detection and Reduction</span></b>
    <div class="gap-10"></div>
    <p>
        Using these observations, we utilize the internal confidence $c_o$ value to classify object presence, since the internal confidence for objects that are not present in the image, or hallucinated, are lower within the image representations.
        We find that utilizing internal confidence to classify object hallucinations provides a 47.17% improvement in mAP in InstructBLIP and 22.45% in LLaVA over the baseline of using the model's output logits directly as a measure of confidence.
    </p>

    <p>
        We use the mass editing technique to remove hallucinations we detected. Evaluating against standard beam search generation, greedy generation, nucleus sampling, and <a href="https://arxiv.org/pdf/2311.17911">OPERA</a>, we find that our method of mass hallucination removal is more effective at reducing hallucinations while preserving the original caption.
    </p>
    
    <div class="gap-10"></div>
    <img width="100%" src="images/hallucination_intervention.png">
    <p></p>
    <p><i><b>Hallucination intervention performance.</b>
        We mass-remove hallucinations that we detect and outperform other baselines. We observe a considerable drop in the raw count of hallucinated objects.</i></p>

    <div class="gap-10"></div>
    <img width="100%" src="images/llava_reduction.png">
    <p></p>
    <p><i><b>Qualitative results for LLaVA hallucination intervention.</b>
        Our algorithm removes hallucinations and, at times, adds correctly detected objects.</i></p>
    
    <div class="gap-10"></div>
    <img width="100%" src="images/blip_reduction.png">
    <p></p>
    <p><i><b>Qualitative results for InstructBLIP hallucination intervention.</b></i></p>

    <hr>

    <b><span style="font-size:25px">Applications: Zero-Shot Segmentation</span></b>
    <div class="gap-10"></div>
    <p>
        We utilize the internal confidence per image feature for zero-shot image segmentation. This application leverages the spatial information encoded in the image representations and demonstrates how VLMs internally represent and localize objects within images.
    </p>
    <img width="100%" src="images/segmentation_table.png">
    <p><i><b>Segmentation Performance on ImageNet-segmentation.</b>
        Localizing objects using their probabilities within the image representations results in more accurate zero-shot segmentation than previous vision-encoders-based and VLM-based methods.</i></p>

    <img width="100%" src="images/zero_shot.png">
    <p><i><b>Zero-shot segmentation.</b>
        Warmer areas indicate higher internal confidence for the class at that image patch. We binarize these values with a threshold to generate segmentations.</i></p>
    <hr>


    <b><span style="font-size:25px">Acknowledgements</span></b>
    <div class="gap-20"></div>
    <p>We thank Kayo Yin for her comments and feedback on our paper. YG is supported by the Google Fellowship. Authors, as part of their affiliation with UC Berkeley, were supported in part by the the Berkeley Artificial Intelligence Research (BAIR) commons program.</p>
  </body>
  </div>

  <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
  <!-- Include all compiled plugins (below), or include individual files as needed -->
  <script src="js/bootstrap.min.js"></script>

</html>
